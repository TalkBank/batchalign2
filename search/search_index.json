{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":"<p>Thanks for stopping by! Batchalign2 is a Python suite of language sample analysis (LSA) software from the TalkBank project. It is used to interact with conversation audio files and their transcripts, and provides a whole host of analyses within this space.</p> <p>The TalkBank Project, of which Batchalign is a part, is supported by NIH grant HD082736.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The following instructions provide a quick start to installing Batchalign. For most users aiming to process CHAT and audio with Batchalign, we recommend more detailed usage instructions: for usage and human transcript cleanup. The following provides a quick start guide for the program.</p>"},{"location":"#install-and-update-the-package","title":"Install and Update the Package","text":"<p>Batchalign is on PyPi (as <code>batchalign</code>). We recommend the use of UV to install Batchalign:</p>"},{"location":"#macos-linux","title":"macOS / Linux","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\nUV_PYTHON=3.11 uv tool install batchalign\n</code></pre>"},{"location":"#windows","title":"Windows","text":"<pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\nuv tool install batchalign\n</code></pre>"},{"location":"#rock-and-roll","title":"Rock and Roll","text":"<p>There are two main ways of interacting with Batchalign. Batchalign can be used as a program to batch-process CHAT (hence the name), or as a Python LSA library.</p> <ul> <li>to get started with the Batchalign program, tap here</li> <li>to get started on the Batchalign Library (assumes familiarity with Python), tap here</li> </ul>"},{"location":"#quick-start-command-line","title":"Quick Start: Command Line","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Once installed, you can invoke the Batchalign program by typing <code>batchalign</code> into the Terminal (MacOS) or Command Prompt (Windows).</p> <p>It is used in the following basic way:</p> <pre><code>batchalign [verb] [input_dir] [output_dir]\n</code></pre> <p>Where <code>verb</code> includes:</p> <ol> <li><code>transcribe</code> - by placing only an audio of video file (<code>.mp3/.mp4/.wav</code>) in the input directory, this function performs ASR on the audio, diarizes utterances, identifies some basic conversational features like retracing and filled pauses, and generates word-level alignments. You must supply a language code flag: <code>--lang=[three letter ISO language code]</code> for the ASR system to know what language the transcript is in. You can choose the flags <code>--rev</code> to use Rev.AI, a commercial ASR service, or <code>--whisper</code>, to use a local copy of OpenAI Whisper. For more information on our Rev.ai integration, see this page.</li> <li><code>align</code> - by placing both an audio of video file (<code>.mp3/.mp4/.wav</code>) and an utterance-aligned CHAT file in the input directory, this function recovers utterance-level time alignments (if they are not already annotated) and generates word-level alignments. The @Languages header in the CHAT file tells the program which language is in the transcript.</li> <li><code>morphotag</code> - by placing a CHAT file in the input directory, this function uses Stanford NLP Stanza to generate morphological and dependency analyses. The @Languages header in the CHAT file tells the program which language is in the transcript. You must supply a language code flag: <code>--lang=[three letter ISO language code]</code> for the alignment system to know what language the transcript is in. </li> </ol> <p>You can get a CHAT transcript to experiment with at the TalkBank website, under any of the \"Banks\" that are available. You can also generate and parse a CHAT transcript via the Python program.</p>"},{"location":"#sample-commands","title":"Sample Commands","text":"<p>For input files (CHAT and audio for <code>align</code>, CHAT only for <code>morphotag</code>, and audio only for <code>transcribe</code>), located in <code>~/ba_input</code> dumping the output to <code>~/ba_output</code>, one could write:</p>"},{"location":"#asr-segmentation","title":"ASR + Segmentation","text":"<pre><code>batchalign transcribe --lang=eng ~/ba_input ~/ba_output\n</code></pre>"},{"location":"#morphosyntactic-analysis","title":"morphosyntactic analysis","text":"<pre><code>batchalign morphotag ~/ba_input ~/ba_output\n</code></pre>"},{"location":"#forced-alignment","title":"forced alignment","text":"<pre><code>batchalign align ~/ba_input ~/ba_output\n</code></pre> <p>Follow instructions from</p> <pre><code>batchalign --help\n</code></pre> <p>and </p> <pre><code>batchalign [verb] --help\n</code></pre> <p>to learn more about other options.</p>"},{"location":"#verbosity","title":"Verbosity","text":"<p>Placing one or multiple <code>-v</code> behind the word <code>batchalign</code> (i.e. behind the <code>[verb]</code> will not work) increases the verbosity of Batchalign. The default mode and one <code>-v</code> will use the normal Batchalign interface, whereas Batchalign with more than 1 <code>-v</code> will switch to the text-based \"logging\" interface.</p> <p>For instance, here is the instruction for running Batchalign to perform forced-alignment:</p> <pre><code>batchalign align input output\n</code></pre> <p>With one <code>-v</code>, you can get stack trace information about any files that crashes: </p> <pre><code>batchalign -v align input output\n</code></pre> <p>and with two <code>-vv</code>, we will ditch the loading bar user interface and instead switch to a logging-based interface that has more information about what Batchalign is doing under the hood:</p> <pre><code>batchalign -vv align input output\n</code></pre>"},{"location":"#quick-start-python","title":"Quick Start: Python","text":"<p>Let's begin!</p> <pre><code>import batchalign as ba\n</code></pre>"},{"location":"#document","title":"Document","text":"<p>The <code>Document</code> is the most basic object in Bachalign. All processing pipelines expect <code>Document</code> as input, and will spit out <code>Document</code> as output.</p> <pre><code>doc = ba.Document.new(\"Hello, this is a transcript! I have two utterances.\", \n                      media_path=\"audio.mp3\", lang=\"eng\")\n\n# navigating the document\nfirst_utterance = doc[0]\nfirst_form = doc[0][0]\nthe_comma = doc[0][1]\n\nassert the_comma.text == ','\nassert the_comma.type == ba.TokenType.PUNCT\n\n# taking a transcript\nsentences = doc.transcript(include_tiers=False, strip=True)\n</code></pre> <p>Notably, if you have a Document that you haven't transcribed yet, you still can make a Document!</p> <pre><code>doc = ba.Document.new(media_path=\"audio.mp3\", lang=\"eng\")\n</code></pre>"},{"location":"#pipelines","title":"Pipelines","text":""},{"location":"#quick-pipeline","title":"Quick Pipeline","text":"<p>Say you wanted to perform ASR, and then tag morphology of the resulting output.</p> <pre><code>nlp = ba.BatchalignPipeline.new(\"asr,morphosyntax\", lang=\"eng\", num_speakers=2)\ndoc = ba.Document.new(media_path=\"audio.mp3\", lang=\"eng\")\ndoc = nlp(doc) # this is equivalent to nlp(\"audio.mp3\"), we will make the initial doc for you\n\nfirst_word_pos = doc[0][0].morphology\nfirst_word_time = doc[0][0].time\nfirst_utterance_time = doc[0].alignment\n</code></pre> <p>The quick API (right now) has support for the following tasks, which you can pass in a comma-separated list in the first argument:</p> <ul> <li><code>asr</code>: ASR!</li> <li><code>morphosyntax</code>: PoS and dependency analysis</li> <li><code>fa</code>: Forced Alignment (require utterance-level timings already)</li> </ul> <p>We will support many, many, many more tasks soon with this API. For now, to gain access to the whole suite of tools, use the second pipeline API discussed below.</p>"},{"location":"#manual-pipeline","title":"Manual Pipeline","text":"<p>Batchalign ships with a plurality of engines which preform the actual processing. For instance, to recreate the demo we had above using the Engines API, we would write</p> <pre><code># ASR\nwhisper = ba.WhisperEngine(lang=\"eng\")\n# retracing and disfluency analysis\nretrace = ba.NgramRetraceEngine()\ndisfluency = ba.DisfluencyReplacementEngine()\n# morphosyntax\nmorphosyntax = ba.StanzaEngine()\n\n# create a pipeline\nnlp = ba.BatchalignPipeline(whisper, retrace, disfluency, morphosyntax)\n\n# and run it!                             \ndoc = nlp(\"audio.mp3\") \n</code></pre> <p>Here's a list of available engines.</p>"},{"location":"#formats","title":"Formats","text":"<p>We currently support reading and writing two transcript formats: TalkBank CHAT, and Praat TextGrid.</p>"},{"location":"#chat","title":"CHAT","text":"<p>Here's how to read and write a CHAT file to parse a TalkBank transcript!</p> <pre><code># reading\nchat = ba.CHATFile(path=\"chat.cha\")\ndoc = chat.doc\n\n# writing\nchat = ba.CHATFile(doc=doc)\nchat.write(\"chat.cha\")\n</code></pre> <p>We will automatically detect audio files located within the same directory as the CHAT file, and associate it with the Batchalign Document.</p>"},{"location":"#textgrid","title":"TextGrid","text":"<p>Importantly, there are two ways a TextGrid could be written: we can either place each utterance in an individual <code>IntervalTier</code>, or each word in its own <code>IntervalTier</code>; we leave that decision up to you. To learn more about TextGrid, visit this page.</p> <pre><code># reading; recall we can either interpret each IntervalTier as a word or utterance\ntg_utterance = ba.TextGridFile(\"utterance\", path=\"tg_ut.TextGrid\", lang=\"eng\")\ntg_word = ba.TextGridFile(\"word\", path=\"tg_w.TextGrid\", lang=\"eng\")\n\ndoc1 = tg_utterance.doc\ndoc2 = tg_word.doc\n\n# writing\ntg_utterance = ba.TextGridFile(\"utterance\", doc=doc1)\ntg_word = ba.TextGridFile(\"word\", doc=doc2)\n\ntg_utterance.write(\"tg_ut.TextGrid\")\ntg_word.write(\"tg_w.TextGrid\")\n</code></pre>"},{"location":"#questions","title":"Questions?","text":"<p>If you have any questions or concerns, please reach out! If something isn't working right, open an issue on GitHub; if you need support, please feel free to email <code>houjun at cmu edu</code> and <code>macw at cmu edu</code>.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Hello! Here we collect some common troubleshooting tips when Batchalign doesn't work. Many of these tips are for OLDER versions of Batchalign, and your best bet is probably to install with the new instructions and everything will likely just work.</p>"},{"location":"troubleshooting/#start-troubleshooting","title":"Start Troubleshooting","text":"<p>First, to confirm your error, run Batchalign in debug mode:</p> <pre><code>batchalign -vvvv [verb] [arguments]\n</code></pre> <p>for instance:</p> <pre><code>batchalign -vvvv align ~/ba_data/input ~/ba_data/output\n</code></pre> <p>If you encounter an error, identify the last line of the Batchalign error output. It should usually be some coloured text placed after the big red box outlining the error location. It usually is shaped <code>[Something]Error: [error message]</code>. </p> <p>Once you have done this, scroll down to the matching error type to learn more.</p>"},{"location":"troubleshooting/#tips","title":"Tips","text":""},{"location":"troubleshooting/#connecttimeout","title":"ConnectTimeout","text":"<pre><code>ConnectionTimeout: (MaxRetryError(\"HTTPSConnectionPool(host='[some website]')\")\n</code></pre> <p>where <code>some website</code> is usually <code>huggingface.co</code> or <code>api.rev.ai</code>. </p> <p>This means we cannot reach the servers\u2014the former, Huggingface, for model serving and the latter, Rev.AI, for ASR. Ensure those websites are accessible from the device where you are running Batchalign + also for your institution.</p>"},{"location":"troubleshooting/#pip-command-not-found-or-pip3-command-not-found","title":"<code>pip: command not found</code> or <code>pip3: command not found</code>","text":"<p>It is unfortunately difficult for us to provide a general command for installation that works across all systems. If your system reports either <code>pip</code> or <code>pip3</code> as not found, and you are sure Python below 3.11 and above 3.8 is installed, try the other <code>pip</code> command. For instance, if you used <code>pip</code>, try using <code>pip3</code>.</p>"},{"location":"troubleshooting/#batchalign-command-not-found-on-windows","title":"<code>batchalign: command not found</code> on Windows","text":"<p>Try using </p> <pre><code>py -m batchalign [verb] [args]\n</code></pre> <p>like:</p> <pre><code>py -m batchalign align ~/ba_data/input ~/ba_data/output\n</code></pre>"},{"location":"troubleshooting/#get-pip-on-windows","title":"Get Pip on Windows","text":"<p>Run the following commands:</p> <ul> <li><code>curl https://bootstrap.pypa.io/ez_setup.py | python</code></li> <li><code>curl https://bootstrap.pypa.io/get-pip.py | python</code></li> </ul> <p>Optionally, you can additionally add the path to your environment so that you can use <code>pip</code> anywhere. It's somewhere like <code>C:\\Python33\\Scripts</code>.</p>"},{"location":"features/rev/","title":"Rev.AI","text":"<p>Batchalign supports ASR transcription via the Rev.AI service, by default via the standard <code>batchalign transcribe</code> command. To use it, you must setup a Rev.AI key.</p>"},{"location":"features/rev/#obtaining-a-revai-key","title":"Obtaining a Rev.AI Key","text":"<p>To run transcribe using Rev-AI, you can sign up for a free account at https://www.rev.ai/auth/signup. Then you sign in and you can get your free access token at https://www.rev.ai/access_token. Copy the token and enter it into a file called rev_key.  During installation, Batchalign will prompt you to enter this key, so you can use ASR from Rev-AI for 5 free hours of audio.  Later you can decide whether or not to switch to a paid subscription using the same key.</p>"},{"location":"features/rev/#privacy","title":"Privacy","text":"<p>If your IRB protocols have privacy concerns, note that when you set up an account with Rev-AI, you can select the option to \u201cnever store media\u201d.</p> <p>They state that \"Rev AI's Automatic Speech Recognition (ASR) and associated API enable covered entities and business associates to utilize Rev AI services in a manner that complies with the Health Insurance Portability and Accountability Act (HIPAA).\"</p>"}]}